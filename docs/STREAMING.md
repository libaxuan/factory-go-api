# ğŸŒŠ æµå¼å“åº”åŠŸèƒ½æ–‡æ¡£

Factory Proxy API å®Œæ•´æ”¯æŒ Server-Sent Events (SSE) æµå¼å“åº”ï¼Œè®©æ‚¨å¯ä»¥å®æ—¶æ¥æ”¶ AI ç”Ÿæˆçš„å†…å®¹ã€‚

---

## ğŸ“– ç›®å½•

- [ä»€ä¹ˆæ˜¯æµå¼å“åº”](#ä»€ä¹ˆæ˜¯æµå¼å“åº”)
- [ä¸ºä»€ä¹ˆä½¿ç”¨æµå¼å“åº”](#ä¸ºä»€ä¹ˆä½¿ç”¨æµå¼å“åº”)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†ç¤ºä¾‹](#è¯¦ç»†ç¤ºä¾‹)
- [SSE æ ¼å¼è¯´æ˜](#sse-æ ¼å¼è¯´æ˜)
- [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
- [æ€§èƒ½è€ƒè™‘](#æ€§èƒ½è€ƒè™‘)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ä»€ä¹ˆæ˜¯æµå¼å“åº”

æµå¼å“åº”ï¼ˆStreamingï¼‰å…è®¸æœåŠ¡å™¨åœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶å®æ—¶å‘é€ç»™å®¢æˆ·ç«¯ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”ç”Ÿæˆåå†ä¸€æ¬¡æ€§è¿”å›ã€‚

### å¯¹æ¯”

| ç‰¹æ€§ | éæµå¼ | æµå¼ |
|------|--------|------|
| **å“åº”æ–¹å¼** | ä¸€æ¬¡æ€§è¿”å›å®Œæ•´å†…å®¹ | å®æ—¶é€å—è¿”å› |
| **ç”¨æˆ·ä½“éªŒ** | ç­‰å¾…æ—¶é—´é•¿ | å³æ—¶åé¦ˆ |
| **é€‚ç”¨åœºæ™¯** | çŸ­æ–‡æœ¬ã€æ‰¹å¤„ç† | é•¿æ–‡æœ¬ã€äº¤äº’å¼å¯¹è¯ |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ç¨å¤æ‚ |

---

## ä¸ºä»€ä¹ˆä½¿ç”¨æµå¼å“åº”

âœ… **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**
- ç”¨æˆ·ç«‹å³çœ‹åˆ°å“åº”å¼€å§‹
- å‡å°‘æ„ŸçŸ¥å»¶è¿Ÿ
- ç±»ä¼¼ ChatGPT çš„æ‰“å­—æœºæ•ˆæœ

âœ… **é€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆ**
- ä»£ç ç”Ÿæˆ
- æ–‡ç« å†™ä½œ
- é•¿ç¯‡å¯¹è¯

âœ… **æ›´å¥½çš„äº¤äº’æ€§**
- ç”¨æˆ·å¯ä»¥æå‰çœ‹åˆ°éƒ¨åˆ†ç»“æœ
- å¯ä»¥æå‰ä¸­æ–­ä¸éœ€è¦çš„ç”Ÿæˆ

---

## å¿«é€Ÿå¼€å§‹

### Python

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_PROXY_API_KEY",
    base_url="http://localhost:8003/v1"
)

# å¯ç”¨æµå¼
stream = client.chat.completions.create(
    model="claude-sonnet-4-5-20250929",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªæ’åºç®—æ³•"}],
    max_tokens=1000,
    stream=True  # ğŸ”‘ å…³é”®å‚æ•°
)

# é€å—å¤„ç†å“åº”
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### Node.js

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env.PROXY_API_KEY,
  baseURL: 'http://localhost:8003/v1'
});

const stream = await client.chat.completions.create({
  model: 'claude-sonnet-4-5-20250929',
  messages: [{ role: 'user', content: 'å†™ä¸€ä¸ªæ’åºç®—æ³•' }],
  max_tokens: 1000,
  stream: true
});

for await (const chunk of stream) {
  if (chunk.choices[0]?.delta?.content) {
    process.stdout.write(chunk.choices[0].delta.content);
  }
}
```

### cURL

```bash
curl -N -X POST http://localhost:8003/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_PROXY_API_KEY" \
  -d '{
    "model": "claude-sonnet-4-5-20250929",
    "messages": [{"role": "user", "content": "å†™ä¸€ä¸ªæ’åºç®—æ³•"}],
    "max_tokens": 1000,
    "stream": true
  }'
```

**æ³¨æ„**: `-N` æˆ– `--no-buffer` å‚æ•°å¾ˆé‡è¦ï¼Œå®ƒç¦ç”¨ç¼“å†²ä»¥å®æ—¶æ˜¾ç¤ºæµå¼å†…å®¹ã€‚

---

## è¯¦ç»†ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€æµå¼å¯¹è¯

```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_PROXY_API_KEY",
    base_url="http://localhost:8003/v1"
)

stream = client.chat.completions.create(
    model="claude-sonnet-4-5-20250929",
    messages=[
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ Python çš„ä¸»è¦ç‰¹ç‚¹"}
    ],
    stream=True
)

print("AI: ", end="", flush=True)
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # æ¢è¡Œ
```

### ç¤ºä¾‹ 2: æ”¶é›†å®Œæ•´å“åº”

```python
stream = client.chat.completions.create(
    model="claude-sonnet-4-5-20250929",
    messages=[{"role": "user", "content": "ç”Ÿæˆä¸€ä¸ª TODO åˆ—è¡¨"}],
    stream=True
)

full_response = ""
for chunk in stream:
    if chunk.choices[0].delta.content:
        content = chunk.choices[0].delta.content
        full_response += content
        print(content, end="", flush=True)

print(f"\n\nå®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
```

### ç¤ºä¾‹ 3: å¸¦ç³»ç»Ÿæç¤ºçš„æµå¼

```python
stream = client.chat.completions.create(
    model="claude-sonnet-4-5-20250929",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python ç¨‹åºå‘˜"},
        {"role": "user", "content": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºçš„å®ç°"}
    ],
    temperature=0.7,
    max_tokens=2000,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## SSE æ ¼å¼è¯´æ˜

æµå¼å“åº”ä½¿ç”¨ **Server-Sent Events (SSE)** æ ¼å¼ã€‚

### å“åº”å¤´

```
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive
```

### æ•°æ®æ ¼å¼

æ¯ä¸ªäº‹ä»¶çš„æ ¼å¼ï¼š

```
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1234567890,"model":"claude-sonnet-4-5-20250929","choices":[{"index":0,"delta":{"role":"assistant"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1234567890,"model":"claude-sonnet-4-5-20250929","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1234567890,"model":"claude-sonnet-4-5-20250929","choices":[{"index":0,"delta":{},"finish_reason":"stop"}]}

data: [DONE]
```

---

## é”™è¯¯å¤„ç†

### å¤„ç†è¿æ¥é”™è¯¯

```python
from openai import OpenAI, OpenAIError

try:
    