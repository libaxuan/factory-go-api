
package main

import (
	"bytes"
	"encoding/json"
	"factory-go-api/config"
	"factory-go-api/transformers"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"strings"
	"time"
)

var startTime = time.Now()

// è·å–ç¯å¢ƒå˜é‡ï¼Œæ”¯æŒé»˜è®¤å€¼
func getEnv(key, defaultValue string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return defaultValue
}

// min å‡½æ•°
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}

// å“åº”è®°å½•å™¨
type responseRecorder struct {
	http.ResponseWriter
	statusCode int
}

func (r *responseRecorder) WriteHeader(code int) {
	r.statusCode = code
	r.ResponseWriter.WriteHeader(code)
}

// å¥åº·æ£€æŸ¥ç«¯ç‚¹
func healthHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":    "healthy",
		"timestamp": time.Now().UTC().Format(time.RFC3339),
		"uptime":    time.Since(startTime).Seconds(),
	})
}

// æ¨¡å‹åˆ—è¡¨ç«¯ç‚¹
func modelsHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	
	models := config.GetAllModels()
	openaiModels := make([]map[string]interface{}, 0, len(models))
	
	for _, model := range models {
		openaiModels = append(openaiModels, map[string]interface{}{
			"id":      model.ID,
			"object":  "model",
			"created": time.Now().Unix(),
			"owned_by": "factory",
		})
	}
	
	json.NewEncoder(w).Encode(map[string]interface{}{
		"object": "list",
		"data":   openaiModels,
	})
}

// API æ–‡æ¡£ç«¯ç‚¹
func docsHandler(w http.ResponseWriter, r *http.Request) {
	htmlContent, err := os.ReadFile("docs.html")
	if err != nil {
		http.Error(w, "Documentation not found", http.StatusNotFound)
		return
	}
	w.Header().Set("Content-Type", "text/html; charset=utf-8")
	w.Write(htmlContent)
}

// OpenAI å…¼å®¹çš„èŠå¤©ç«¯ç‚¹
func chatCompletionsHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, `{"error": "Method not allowed"}`, http.StatusMethodNotAllowed)
		return
	}

	// è·å–å®¢æˆ·ç«¯ Authorization å¤´
	authHeader := r.Header.Get("Authorization")
	if authHeader == "" {
		http.Error(w, `{"error": {"message": "Authorization header is required", "type": "invalid_request_error"}}`, http.StatusUnauthorized)
		return
	}

	// éªŒè¯ PROXY_API_KEYï¼ˆå¦‚æœé…ç½®äº†ï¼‰
	proxyAPIKey := getEnv("PROXY_API_KEY", "")
	if proxyAPIKey != "" {
		// æå–å®¢æˆ·ç«¯ API Key
		parts := strings.SplitN(authHeader, " ", 2)
		if len(parts) != 2 || parts[0] != "Bearer" {
			http.Error(w, `{"error": {"message": "Invalid authorization header format", "type": "invalid_request_error"}}`, http.StatusUnauthorized)
			return
		}
		clientAPIKey := parts[1]

		// éªŒè¯å®¢æˆ·ç«¯ Key æ˜¯å¦åŒ¹é…ä»£ç† Key
		if clientAPIKey != proxyAPIKey {
			log.Printf("âŒ API Key éªŒè¯å¤±è´¥")
			http.Error(w, `{"error": {"message": "Invalid API key", "type": "authentication_error"}}`, http.StatusUnauthorized)
			return
		}
	}

	// ä½¿ç”¨æºå¤´ FACTORY_API_KEY æ›¿æ¢ Authorization å¤´
	factoryAPIKey := getEnv("FACTORY_API_KEY", "")
	if factoryAPIKey == "" {
		log.Printf("âŒ FACTORY_API_KEY æœªé…ç½®")
		http.Error(w, `{"error": {"message": "Server configuration error", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}
	authHeader = "Bearer " + factoryAPIKey

	// è¯»å–è¯·æ±‚ä½“
	bodyBytes, err := io.ReadAll(r.Body)
	if err != nil {
		log.Printf("é”™è¯¯: è¯»å–è¯·æ±‚ä½“å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to read request body", "type": "invalid_request_error"}}`, http.StatusBadRequest)
		return
	}
	defer r.Body.Close()

	// è§£æ OpenAI è¯·æ±‚
	var openaiReq transformers.OpenAIRequest
	if err := json.Unmarshal(bodyBytes, &openaiReq); err != nil {
		log.Printf("é”™è¯¯: è§£æè¯·æ±‚ä½“å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Invalid JSON", "type": "invalid_request_error"}}`, http.StatusBadRequest)
		return
	}

	// æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
	model := config.GetModelByID(openaiReq.Model)
	if model == nil {
		log.Printf("âŒ ä¸æ”¯æŒçš„æ¨¡å‹: %s", openaiReq.Model)
		http.Error(w, fmt.Sprintf(`{"error": {"message": "Model '%s' not found", "type": "invalid_request_error"}}`, openaiReq.Model), http.StatusNotFound)
		return
	}

	log.Printf("âœ… %s [%s] stream=%v", openaiReq.Model, model.Type, openaiReq.Stream)

	// æ ¹æ®æ¨¡å‹ç±»å‹è·¯ç”±è¯·æ±‚
	switch model.Type {
	case "anthropic":
		handleAnthropicRequest(w, r, &openaiReq, model, authHeader)
	case "openai":
		handleFactoryOpenAIRequest(w, r, &openaiReq, model, authHeader)
	default:
		http.Error(w, `{"error": {"message": "Unsupported model type", "type": "invalid_request_error"}}`, http.StatusBadRequest)
	}
}

// å¤„ç† Anthropic ç±»å‹è¯·æ±‚
func handleAnthropicRequest(w http.ResponseWriter, r *http.Request, openaiReq *transformers.OpenAIRequest, model *config.Model, authHeader string) {
	// è½¬æ¢è¯·æ±‚
	anthropicReq := transformers.TransformToAnthropic(openaiReq)
	
	// è·å–ç«¯ç‚¹
	endpoint := config.GetEndpointByType("anthropic")
	if endpoint == nil {
		http.Error(w, `{"error": {"message": "Anthropic endpoint not configured", "type": "configuration_error"}}`, http.StatusInternalServerError)
		return
	}

	// åºåˆ—åŒ–è¯·æ±‚
	reqBody, err := json.Marshal(anthropicReq)
	if err != nil {
		log.Printf("é”™è¯¯: åºåˆ—åŒ–è¯·æ±‚å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to serialize request", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}


	// åˆ›å»º HTTP è¯·æ±‚
	proxyReq, err := http.NewRequest(http.MethodPost, endpoint.BaseURL, bytes.NewBuffer(reqBody))
	if err != nil {
		log.Printf("é”™è¯¯: åˆ›å»ºè¯·æ±‚å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to create request", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// è®¾ç½®è¯·æ±‚å¤´
	clientHeaders := extractClientHeaders(r)
	headers := transformers.GetAnthropicHeaders(authHeader, clientHeaders, openaiReq.Stream, model.ID)
	for key, value := range headers {
		proxyReq.Header.Set(key, value)
	}

	// å‘é€è¯·æ±‚
	client := &http.Client{Timeout: 120 * time.Second}
	resp, err := client.Do(proxyReq)
	if err != nil {
		log.Printf("é”™è¯¯: è¯·æ±‚å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Request to upstream failed", "type": "upstream_error"}}`, http.StatusBadGateway)
		return
	}
	defer resp.Body.Close()

	log.Printf("ğŸ“¥ Anthropic å“åº”: %d", resp.StatusCode)

	// å¤„ç†å“åº”
	if openaiReq.Stream {
		// æµå¼å“åº”
		handleAnthropicStreamResponse(w, resp, model.ID)
	} else {
		// éæµå¼å“åº”
		handleAnthropicNonStreamResponse(w, resp, model.ID)
	}
}

// å¤„ç† Factory OpenAI ç±»å‹è¯·æ±‚
func handleFactoryOpenAIRequest(w http.ResponseWriter, r *http.Request, openaiReq *transformers.OpenAIRequest, model *config.Model, authHeader string) {
	// è½¬æ¢è¯·æ±‚
	factoryReq := transformers.TransformToFactoryOpenAI(openaiReq)
	
	// è·å–ç«¯ç‚¹
	endpoint := config.GetEndpointByType("openai")
	if endpoint == nil {
		http.Error(w, `{"error": {"message": "OpenAI endpoint not configured", "type": "configuration_error"}}`, http.StatusInternalServerError)
		return
	}

	// åºåˆ—åŒ–è¯·æ±‚
	reqBody, err := json.Marshal(factoryReq)
	if err != nil {
		log.Printf("é”™è¯¯: åºåˆ—åŒ–è¯·æ±‚å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to serialize request", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}


	// åˆ›å»º HTTP è¯·æ±‚
	proxyReq, err := http.NewRequest(http.MethodPost, endpoint.BaseURL, bytes.NewBuffer(reqBody))
	if err != nil {
		log.Printf("é”™è¯¯: åˆ›å»ºè¯·æ±‚å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to create request", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// è®¾ç½®è¯·æ±‚å¤´
	clientHeaders := extractClientHeaders(r)
	headers := transformers.GetFactoryOpenAIHeaders(authHeader, clientHeaders)
	for key, value := range headers {
		proxyReq.Header.Set(key, value)
	}

	// å‘é€è¯·æ±‚
	client := &http.Client{Timeout: 120 * time.Second}
	resp, err := client.Do(proxyReq)
	if err != nil {
		log.Printf("é”™è¯¯: è¯·æ±‚å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Request to upstream failed", "type": 
"upstream_error"}}`, http.StatusBadGateway)
		return
	}
	defer resp.Body.Close()

	log.Printf("ğŸ“¥ Factory OpenAI å“åº”: %d", resp.StatusCode)

	// å¤„ç†å“åº”
	if openaiReq.Stream {
		// æµå¼å“åº”
		handleFactoryOpenAIStreamResponse(w, resp, model.ID)
	} else {
		// éæµå¼å“åº”
		handleFactoryOpenAINonStreamResponse(w, resp, model.ID)
	}
}

// å¤„ç† Anthropic éæµå¼å“åº”
func handleAnthropicNonStreamResponse(w http.ResponseWriter, resp *http.Response, modelID string) {
	// è¯»å–å“åº”ä½“
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		log.Printf("é”™è¯¯: è¯»å–å“åº”å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to read response", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	if resp.StatusCode != http.StatusOK {
		// é”™è¯¯å“åº”ç›´æ¥è½¬å‘
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(resp.StatusCode)
		w.Write(body)
		return
	}

	// è§£æ Anthropic å“åº”
	var anthropicResp map[string]interface{}
	if err := json.Unmarshal(body, &anthropicResp); err != nil {
		log.Printf("é”™è¯¯: è§£æå“åº”å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to parse response", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// è½¬æ¢ä¸º OpenAI æ ¼å¼
	transformer := transformers.NewAnthropicResponseTransformer(modelID, "")
	openaiResp, err := transformer.TransformNonStreamResponse(anthropicResp)
	if err != nil {
		log.Printf("é”™è¯¯: è½¬æ¢å“åº”å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to transform response", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// è¿”å› OpenAI æ ¼å¼å“åº”
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(openaiResp)
}

// å¤„ç† Anthropic æµå¼å“åº”
func handleAnthropicStreamResponse(w http.ResponseWriter, resp *http.Response, modelID string) {
	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")

	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, `{"error": {"message": "Streaming not supported", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// åˆ›å»ºè½¬æ¢å™¨
	transformer := transformers.NewAnthropicResponseTransformer(modelID, "")
	
	// è½¬æ¢æµå¼å“åº”
	outputChan := transformer.TransformStream(resp.Body)
	
	for chunk := range outputChan {
		fmt.Fprint(w, chunk)
		flusher.Flush()
	}
}

// å¤„ç† Factory OpenAI éæµå¼å“åº”
func handleFactoryOpenAINonStreamResponse(w http.ResponseWriter, resp *http.Response, modelID string) {
	// è¯»å–å“åº”ä½“
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		log.Printf("é”™è¯¯: è¯»å–å“åº”å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to read response", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	if resp.StatusCode != http.StatusOK {
		// é”™è¯¯å“åº”ç›´æ¥è½¬å‘
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(resp.StatusCode)
		w.Write(body)
		return
	}

	// è§£æ Factory OpenAI å“åº”
	var factoryResp map[string]interface{}
	if err := json.Unmarshal(body, &factoryResp); err != nil {
		log.Printf("é”™è¯¯: è§£æå“åº”å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to parse response", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// è½¬æ¢ä¸º OpenAI æ ¼å¼
	transformer := transformers.NewFactoryOpenAIResponseTransformer(modelID, "")
	openaiResp, err := transformer.TransformNonStreamResponse(factoryResp)
	if err != nil {
		log.Printf("é”™è¯¯: è½¬æ¢å“åº”å¤±è´¥: %v", err)
		http.Error(w, `{"error": {"message": "Failed to transform response", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// è¿”å› OpenAI æ ¼å¼å“åº”
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(openaiResp)
}

// å¤„ç† Factory OpenAI æµå¼å“åº”
func handleFactoryOpenAIStreamResponse(w http.ResponseWriter, resp *http.Response, modelID string) {
	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")

	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, `{"error": {"message": "Streaming not supported", "type": "server_error"}}`, http.StatusInternalServerError)
		return
	}

	// åˆ›å»ºè½¬æ¢å™¨
	transformer := transformers.NewFactoryOpenAIResponseTransformer(modelID, "")
	
	// è½¬æ¢æµå¼å“åº”
	outputChan := transformer.TransformStream(resp.Body)
	
	for chunk := range outputChan {
		fmt.Fprint(w, chunk)
		flusher.Flush()
	}
}

// æå–å®¢æˆ·ç«¯è¯·æ±‚å¤´
func extractClientHeaders(r *http.Request) map[string]string {
	headers := make(map[string]string)
	
	// æå–éœ€è¦è½¬å‘çš„å¤´
	forwardHeaders := []string{
		"x-session-id",
		"x-assistant-message-id",
		"x-factory-client",
		"x-stainless-arch",
		"x-stainless-lang",
		"x-stainless-os",
		"x-stainless-runtime",
		"x-stainless-retry-count",
		"x-stainless-package-version",
		"x-stainless-runtime-version",
	}
	
	for _, header := range forwardHeaders {
		if value := r.Header.Get(header); value != "" {
			headers[header] = value
		}
	}
	
	return headers
}

func main() {
	// éªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡
	factoryAPIKey := getEnv("FACTORY_API_KEY", "")
	if factoryAPIKey == "" {
		log.Fatalf("âŒ é”™è¯¯: å¿…é¡»è®¾ç½® FACTORY_API_KEY ç¯å¢ƒå˜é‡")
	}

	proxyAPIKey := getEnv("PROXY_API_KEY", "")
	if proxyAPIKey != "" {
		log.Printf("ğŸ” ä»£ç†æ¨¡å¼: å·²å¯ç”¨")
		log.Printf("   â€¢ å¯¹å¤– Key: %s", proxyAPIKey)
		log.Printf("   â€¢ æºå¤´ Key: %s***", factoryAPIKey[:min(8, len(factoryAPIKey))])
	} else {
		log.Printf("ğŸ” ç›´è¿æ¨¡å¼: ä½¿ç”¨ FACTORY_API_KEY ç›´æ¥è®¿é—®")
	}

	// åŠ è½½é…ç½®
	configPath := getEnv("CONFIG_PATH", "config.json")
	log.Printf("ğŸ“– åŠ è½½é…ç½®æ–‡ä»¶: %s", configPath)
	
	cfg, err := config.LoadConfig(configPath)
	if err != nil {
		log.Fatalf("âŒ åŠ è½½é…ç½®å¤±è´¥: %v", err)
	}

	log.Printf("âœ… é…ç½®åŠ è½½æˆåŠŸ")
	log.Printf("ğŸ“ æ”¯æŒçš„æ¨¡å‹ (%d):", len(cfg.Models))
	for _, model := range cfg.Models {
		log.Printf("   â€¢ %s [%s]", model.ID, model.Type)
	}

	// è®¾ç½®è·¯ç”±
	http.HandleFunc("/health", healthHandler)
	http.HandleFunc("/v1/models", modelsHandler)
	http.HandleFunc("/v1/chat/completions", chatCompletionsHandler)
	http.HandleFunc("/docs", docsHandler)
	
	// æ ¹è·¯å¾„
	http.HandleFunc("/", func(w http.ResponseWriter, r *http.Request) {
		if r.URL.Path != "/" {
			http.Error(w, `{"error": {"message": "Not found", "type": "invalid_request_error"}}`, http.StatusNotFound)
			return
		}
		
		w.Header().Set("Content-Type", "application/json")
		json.NewEncoder(w).Encode(map[string]interface{}{
			"service": "Factory Go API - Multi-Model Support",
			"version": "2.0",
			"endpoints": []string{
				"/health",
				"/v1/models",
				"/v1/chat/completions",
			},
		})
	})

	// å¯åŠ¨æœåŠ¡å™¨
	port := fmt.Sprintf(":%d", cfg.Port)
	server := &http.Server{
		Addr:         port,
		ReadTimeout:  120 * time.Second,
		WriteTimeout: 300 * time.Second,
		
IdleTimeout:  120 * time.Second,
	}

	log.Printf("ğŸš€ æœåŠ¡å¯åŠ¨äº http://localhost%s", port)
	log.Printf("ğŸ“– æ–‡æ¡£: http://localhost%s/docs", port)
	
	if err := server.ListenAndServe(); err != nil {
		log.Fatalf("âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: %v", err)
	}
}