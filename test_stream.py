#!/usr/bin/env python3
"""
æµ‹è¯•æµå¼å’Œéæµå¼å“åº”åŠŸèƒ½

ä¾èµ–å®‰è£…:
    pip install openai
"""

import os
import sys

try:
    from openai import OpenAI
except ImportError:
    print("âŒ é”™è¯¯: æœªå®‰è£… openai åº“")
    print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–:")
    print("  pip install openai")
    print("\næˆ–è€…:")
    print("  pip3 install openai")
    sys.exit(1)

# é…ç½®
API_KEY = os.getenv("PROXY_API_KEY", "your_proxy_api_key")
BASE_URL = "http://localhost:8003/v1"
MODEL = "claude-sonnet-4-5-20250929"

client = OpenAI(
    api_key=API_KEY,
    base_url=BASE_URL
)

def test_non_stream():
    """æµ‹è¯•éæµå¼å“åº”"""
    print("=" * 60)
    print("æµ‹è¯• 1: éæµå¼å“åº”")
    print("=" * 60)
    
    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}
            ],
            max_tokens=100,
            stream=False
        )
        
        print(f"âœ… éæµå¼å“åº”æˆåŠŸ")
        print(f"æ¨¡å‹: {response.model}")
        print(f"å†…å®¹: {response.choices[0].message.content}")
        print(f"Token ä½¿ç”¨: {response.usage}")
        print()
        
    except Exception as e:
        print(f"âŒ éæµå¼å“åº”å¤±è´¥: {e}")
        print()

def test_stream():
    """æµ‹è¯•æµå¼å“åº”"""
    print("=" * 60)
    print("æµ‹è¯• 2: æµå¼å“åº”")
    print("=" * 60)
    
    try:
        stream = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»è‡ªå·±"}
            ],
            max_tokens=100,
            stream=True
        )
        
        print("âœ… æµå¼å“åº”æˆåŠŸï¼Œæ¥æ”¶å†…å®¹:")
        print("-" * 60)
        
        content = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                text = chunk.choices[0].delta.content
                content += text
                print(text, end="", flush=True)
        
        print()
        print("-" * 60)
        print(f"å®Œæ•´å†…å®¹: {content}")
        print()
        
    except Exception as e:
        print(f"âŒ æµå¼å“åº”å¤±è´¥: {e}")
        print()

def test_stream_with_system():
    """æµ‹è¯•å¸¦ç³»ç»Ÿæç¤ºçš„æµå¼å“åº”"""
    print("=" * 60)
    print("æµ‹è¯• 3: æµå¼å“åº” + ç³»ç»Ÿæç¤º")
    print("=" * 60)
    
    try:
        stream = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python ç¼–ç¨‹åŠ©æ‰‹"},
                {"role": "user", "content": "å†™ä¸€ä¸ª Hello World ç¨‹åº"}
            ],
            max_tokens=200,
            temperature=0.7,
            stream=True
        )
        
        print("âœ… æµå¼å“åº”æˆåŠŸï¼Œæ¥æ”¶å†…å®¹:")
        print("-" * 60)
        
        content = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                text = chunk.choices[0].delta.content
                content += text
                print(text, end="", flush=True)
        
        print()
        print("-" * 60)
        print(f"å®Œæ•´å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        print()
        
    except Exception as e:
        print(f"âŒ æµå¼å“åº”å¤±è´¥: {e}")
        print()

if __name__ == "__main__":
    print("\nğŸš€ Factory Proxy API - æµå¼/éæµå¼æµ‹è¯•\n")
    
    # æµ‹è¯•éæµå¼
    test_non_stream()
    
    # æµ‹è¯•æµå¼
    test_stream()
    
    # æµ‹è¯•æµå¼ + ç³»ç»Ÿæç¤º
    test_stream_with_system()
    
    print("=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    print("=" * 60)